# ReluDiff

ReluDiff를 인용하고 있는 논문들에서 ReluDiff를 어떻게 언급하고 있는지 모아둔 문서.
대부분 DNN Verification의 예시로서 ReluDiff을 언급. (예. DNN robustness 성질을 증명하는 verification works들이 있었다.)

아래 인용들 중 ReluDiff에 대한 유의미한 언급에는 볼드체 적용함.

출처를 안 적은 것은 아카이브 논문

### Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance (ASE '20)

- We perform our experiments using three
  popular datasets: MNIST [65], CIFAR10 [61], and CIFAR100 [61].
  We choose image classification architectures as they are often used
  as test subjects in recent SE papers that test [41, 71, 73, 85, 100,
  105, 106, 109, 115, 120], verify [46, 83 **(ReluDiff)**], and improve [39, 56, 74, 110,
  113, 116] DL models and libraries.

### DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection (ICSE '21)

- Unlike the adversarial attack that is widely-known
  as a robustness issue and many methods have been proposed
  to test [16], [17], [18], [19], enhance [20], or verify [21 **(ReluDiff)**] the
  robustness, the potential influence of backdoor attacks is not
  well understood.

### ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing (ICSE '22)

- Adversarial vulnerability means that a DNN is vulnerable to
  adversarial inputs that are generated by adding special noise to normal inputs and lead to prediction errors [26]. The most common way
  to generate adversarial inputs is to use gradient ascent techniques on
  the white-box model [48]. In the software engineering community,
  this vulnerability is viewed as a robustness issue, and many techniques have been proposed to test [21, 55, 65], improve [15, 17], or
  verify [7, 43, 54 **(ReluDiff)**, 60] the DNN robustness.

### Are Machine Learning Cloud APIs Used Correctly? (ICSE '21)

- Some research studies common mistakes in programs that
  design and train neural networks [16]-[19] or other types
  of machine learning models (e.g., SVM and decision tree)
  [93]. Some works focus on testing [20]-[45] **(ReluDiff)** and fixing [46]-
  [49] neural networks. All of these studies consider building
  machine learning models, instead of using them.

### Automated Testing of Software that Uses Machine Learning APIs (ICSE '22)

- Much research has been done for testing [9,
  17–41 **(ReluDiff)**, 92] and fixing [42–45] neural networks, in terms of accuracy,
  fairness, and security.

### QVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks (ASE '22)

- Almost all the existing work is designated for real or floating point numbered DNNs only whereas verification of quantized DNNs
  (QNNs) has not been thoroughly investigated yet, although there
  is a gap of verification results between real-numbered DNNs and
  their quantized counterparts due to the fixed-point semantics of
  QNNs [24]. Thus, there is a growing need for the research of quality
  assurance techniques for QNNs. One possible approach to verify
  QNNs is to adopt differential verification [41] which was initially
  proposed for verifying a new version of a program with respect to
  a previous version. One could first verify a real or floating-point
  numbered DNN by applying existing techniques and then verify
  its quantized counterpart by applying differential verification techniques for DNNs [50, 59 **(ReluDiff)**, 60].
  However, it has two drawbacks. First,
  **existing differential verification techniques for DNNs [50, 59, 60] are
  incomplete, and thus may produce false positives even if the original
  DNN is robust.** Second, quantization introduces non-monotonicity
  on the output of DNNs [24], consequently, **a robust DNN may become non-robust after quantization while a non-robust DNN may
  become a robust QNN. Therefore, dedicated techniques are required
  for directly and rigorously verifying QNNs.**

### Example Guided Synthesis of Linear Approximations for Neural Network Verification (CAV '22)

- As a result, a large number of works have proposed verification techniques to prove that a neural network is not vulnerable to these perturba- tions [35,43,44], or in general satisfies some specification [15,18,27 **(ReluDiff)**,28].

### NeuroDiff: Scalable Differential Verification of Neural Networks using Fine-Grained Approximation (ASE '20) - ReluDiff과 동일 저자

- Unfortunately, most work aimed at verifying or testing neural
  networks does not provide formal guarantees on their equivalence.
  For example, testing techniques geared toward refutation can provide inputs where a single network misbehaves [22, 31, 42, 44, 51] or
  multiple networks disagree [23, 34, 52], but they do not guarantee
  the absence of misbehaviors or disagreements. While techniques
  geared toward verification can prove safety or robustness properties of a single network [7–9, 15, 18, 25, 38, 41, 47], they lack
  crucial information needed to prove the equivalence of multiple
  networks. One exception is the ReluDiff tool of Paulsen et al. [33],
  which computes a sound approximation of the difference of two
  neural networks, a problem known as differential verification. While
  ReluDiff performs better than other techniques, the overly conservative approximation it computes often causes both accuracy
  and efficiency to suffer.

### LinSyn: Synthesizing Tight Linear Bounds for Arbitrary Neural Network Activation Functions? (TACAS '22)

- Linear Bound-based Neural Network Verification There is a large body of work
  on using linear-bounding techniques [36, 48, 34, 6, 45, 29 **(ReluDiff)**, 30, 46, 27] and
  other abstract domains such as concrete intervals, symbolic intervals [44], and
  Zonotopes [15], for the purpose of neural network verification.

### Can Differential Testing Improve Automatic Speech Recognition Systems? (ICSME '21)

- Differential testing has shown to be effective in revealing failures in deep learning-powered systems,
  e.g., speech recognition [1], [2], object detection [4], [5] and
  human activity recognition [6 **(ReluDiff)**]. : Evaluation에서 human activity data도 사용해서 인용한 듯

### DIFFRNN: DIFFERENTIAL VERIFICATION OF RECURRENT NEURAL NETWORKS

- To the best of our knowledge, RELUDIFF [18] is the
  only tool that aims to prove the equivalence of two neural networks for all inputs. RELUDIFF takes as input two
  feed-forward neural networks with piecewise linear activation functions known as rectified linear units ( ReLU). The
  ReLU activation essentially allows the neural network to be treated as a piecewise linear (PWL) function (with possibly
  many facets/pieces).

- RELUDIFF [18] is currently the only tool that can verify neural networks in the differential setting. However, unlike our
  approach, RELUDIFF does not solve the many challenges that are unique to RNNs

### Reducing DNN Properties to Enable Falsification with Adversarial Attacks (ICSE '21)

- **Differential properties are the most recently introduced
  DNN property type [42].** These properties specify a difference
  (or lack thereof) between outputs of multiple DNNs. One
  type of differential property is equivalence, which states that
  for every input, two DNN models produce the same output.
  **Such a property can be used to check that DNN semantics
  are preserved after some modification, such as quantization or
  pruning. Differential properties can be supported by combining
  multiple DNNs into a single network and expressing properties
  over their combined input and output domains.**

- The ReluDiff verifier was designed to support differential
  properties in order to show equivalance between two networks [42].

- Finally, on the CIFAR-EQ benchmark, the verifiers did not
  find any violations because they could not be run. Reluplex,
  Planet, ERAN, and Neurify do not support properties over
  multiple networks or networks with multiple computation
  paths, while ReluDiff is limited to networks with only fullyconnected layers.

### Proof Transfer for Fast Certification of Multiple Approximate Neural Networks (OOPSLA '22)

- **Differential neural network verification is a related concept that aims to bound the difference
  between the output of original and approximate networks [Paulsen et al. 2020a,b]. However, this
  approach and the solving mechanisms cannot be directly used to prove the robustness properties
  of the approximate network.** : why? 설명이 없음. 아마 Quantization만 다룰 수 있어서?

- 이 논문에서 말하는 approximate network : Deploying DNNs on real-world systems has opened the question
  of optimizing the computational cost of inference: to reduce this cost, researchers have devised
  various techniques for approximating DNNs, which simplify the structure of the network (typically
  post network training), while maintaining high accuracy and robustness. Common approximation
  techniques include **quantization** (reducing numerical precision of the weights) [Gholami et al.
  2021], **pruning** (removing weights or groups of weights) [Frankle and Carbin 2019], **operator
  approximation** (e.g. approximating convolutions [Figurnov et al. 2016; Sharif et al. 2021]) and others.
  Further, even after deployment, the developers may need to re-optimize the neural network if they
  detect a distribution shift [Rabanser et al. 2019].

- 기존 network의 성질 증명과 Proof Transfer를 이용해 Approximated network를 더 빠르게 성질 증명

### Shared Certificates for Neural Network Verification (CAV '22)

- Proof Sharing Between Networks: In neural network verification, some approaches abstract the network to achieve speed-ups in verification. These sim- plifications are constructed in a way that the proof can be adapted for the original neural network [1,43]. Similarly, another family of approaches analyzes the dif- ference between two closely related neural networks by utilizing their structural similarity [26 **(ReluDiff)**,27]. Such approaches can be used to reuse analysis results between neural network modifications, e.g. fine-tuning [9,37].
- In contrast to these works, we do not modify the neural network, but achieve speed-ups rather by only considering the relaxations obtained in the proofs. [37] additionally consider small changes to the input, however, these are much smaller than the difference in specification we consider here.

### Towards Practical Robustness Analysis for DNNs based on PAC-Model Learning (ICSE '22)

- A number of formal verification techniques
  have been proposed for DNNs, including constraint-solving [8, 16,
  19, 22, 24, 32, 39, 47], abstract interpretation [21, 37, 59, 60, 84], layerby-layer exhaustive search [29], global optimisation [15, 55, 56], convex relaxation [31, 49 **(ReluDiff)**, 50], functional approximation [76], reduction
  to two-player games [77, 79], and star-set-based abstraction [66, 67].
  Sampling-based methods are adopted to probabilistic robustness
  verification in [2, 3, 12, 45, 74, 75].

### Knowledge Augmented Machine Learning with Applications in Autonomous Driving: A Survey

- There are some works that have lifted the verification concept
  to the analysis of differences of neural networks, for example,
  in order to verify the accuracy of a compressed network. For
  the original (deep) neural network f and the compressed
  counterpart f' , the verification amounts to showing that |f(x) − f'(x)| < _eps_ for all x in the input space. The authors
  in [615] use symbolic interval analysis for analyzing differences in the values of neurons and gradients and call their
  algorithm ReluDiff which over-approximates the respective
  differences and checks in a forward pass if the resulting
  output difference exceeds _eps_.

### Systematic Testing of the Data-Poisoning Robustness of KNN (ISSTA '23)

- At a higher level, our method for using over-approximate analysis to narrow down the search space is analogous to static analysis
  techniques based on abstract interpretation [11], which have been
  used to verify properties of both software programs [28, 48, 53]
  and machine learning models [40–42 **(ReluDiff)**], including robustness to data
  bias [37] and individual fairness [32].

### Incremental Verification of Neural Networks (PLDI '23)

- ReluDiff [Paulsen et al. 2020a] presented the concept
  of dierential neural network verication. The follow-up work of [Paulsen et al. 2020b] made it
  more scalable. ReluDiff can be used for bounding the dierence in the output of an original network
  and a perturbed network corresponding to an input region. ReluDiff uses input splitting to perform
  complete dierential verication. **Our method is complementary to ReluDiff and can be used to
  speed up the complete dierential verication with multiple perturbed networks, performing it
  incrementally.**

### QEBVerif: Quantization Error Bound Verification of Neural Networks

- **Naively, one could use an existing verification tool in the literature to independently compute the output intervals for both the QNN and the DNN, and then
  compute their output difference directly by interval subtraction. However, such
  an approach would be ineffective due to the significant precision loss.**
  Recently, Paulsen et al. [48] proposed ReluDiff and showed that the accuracy of output difference for two DNNs can be greatly improved by propagating
  the difference intervals layer-by-layer. For each hidden layer, they first compute
  the output difference of affine functions (before applying the ReLU), and then
  they use a ReLU transformer to compute the output difference after applying the
  ReLU functions. **The reason why ReluDiff outperforms the naive method is
  that ReluDiff first computes part of the difference before it accumulates. ReluDiff is later improved to tighten the approximated difference intervals [49].
  However, as mentioned previously, they do not support fully quantified neural
  networks.** Inspired by their work, we design a difference propagation algorithm
  for our setting.

### Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning

- There has been significant recent interest in exploring techniques to enhance the
  safety of learning-enabled systems. Techniques described in [20 **(ReluDiff)**,23,15,16] define suitable verification methodologies that are capable of providing stronger guarantees for open-loop LECs.

### Towards Repairing Neural Networks Correctly

- The second is on DNN testing [3], [35].
  Inspired by the traditional software testing, this thread of
  research detects DNNs’ incorrect behaviors by testing, such
  as DeepXplore [23], ReluDiff [22] and DeepGini [4].

### Continuous Safety Verification of Neural Networks

- Lastly, an interesting problem
  related to ours is a recent work to check the difference
  of two DNNs [20], which provides formal guarantees for
  the relationship between two networks with forward interval
  analysis and backward refinement.

### Fairify: Fairness Verification of Neural Networks

- In practice, the ReLU activation is widely used
  as the NL function since it demonstrates good performance,
  and following the prior works in the area [27, 32, 41, 42], we
  also considered ReLU based NN.

### Certifying the Fairness of KNN in the Presence of Dataset Bias(CAV'23)

- Since fairness is a type of non-functional property, the verification/certification techniques are often significantly
  different from techniques used to verify/certify functional correctness. Instead, they are more closely related to techniques
  for verifying/certifying robustness [8], noninterference [5], and side-channel
  security [19, 39, 40, 48], where a program is executed multiple times, each time
  for a different input drawn from a large (and sometimes infinite) set, to see if
  they all agree on the output. At a high level, this is closely related to differential
  verification [28, 31 **(ReluDiff)**, 32], synthesis of relational invariants [41] and verification of
  hyper-properties [15, 35].

### Geometric Path Enumeration for Equivalence Verification of Neural Networks (ICTAI '21)

- While Paulsen et al. [8] proposed a technique
  called RELUDIFF/NEURODIFF which uses (symbolic) interval
  propagation on NNs with similar weight configurations (e.g.
  produced through float truncation) to prove -equivalence, the
  MILPEQUIV technique [7] is based on mixed integer linear
  programming (MILP) and encodes (potentially) structurally
  different NNs together with the desired equivalence property
  into an optimization problem. RELUDIFF/NEURODIFF was shown to be efficient for cases where a NN’s weights had
  been truncated. **However, this approach does not work at all
  for structurally differing NNs and suffers in performance when
  weight differences are larger.**

### Efficient generation of valid test inputs for deep neural networks via gradient search (Softw Evol Proc. 2023?)

- Previous researchers have proposed many test generation methods for DNNs, such as differential testing (DeepXplore and ReluDiff
  ), mutation testing (DeepMutation), fuzzy testing (DLFuzz, DeepHunter, and SENSEI) and concolic testing (DeepConcolic,), and a series of test adequacy
  criteria have been designed.

### On Neural Network Equivalence Checking using SMT Solvers ('22)

- Moreover, neural network models in systems or programs with learning-enabled components [3] may have to
  be updated for a number of reasons [16 **(ReluDiff)**]; for example, security concerns such as the need to withstand data
  perturbations (e.g. adversarial examples), or possibly incomplete coverage of the neural network’s input
  domain.
- Worth to mention is the work in [16], where the authors focus on analyzing the relationship between two
  neural networks, e.g. whether a modified version of an existing neural network produces outputs within some
  bound relative to the original network. **While the focus is not to answer the question of equivalence given
  an appropriate equivalence criterion, the authors propose an interesting “differential verification” technique
  that consists of a forward interval analysis through the network’s layers, followed by a backward pass that
  iteratively refines the approximation, until having verified the property of interest.**

- Neural Network Equivalence Checking? Diff verification은 비슷한 두 모델의 성질. Equi checking은 그냥 다른 두 모델이 같음 이라는 성질

### A Declarative Metamorphic Testing Framework for Autonomous Driving (IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, '23)

- There is also a large body of verification techniques for
  deep learning models [49], [50], [51], [52], [53], [54], [55],
  [56], [57].

### Verifying Global Neural Network Specifications using Hyperproperties ('23)

- Improved encodings of self-composition [36] and approaches from differential verification of neural networks [26] are interesting directions for improving
  the verification of NNDHs.

### Understanding the Complexity and Its Impact on Testing in ML-Enabled Systems ('23)

- For example, many advances have been made for DL model testing (e.g., [4, 19, 25,
  40, 59, 68, 72, 82, 84]), verification (e.g., [9, 57, 58, 66, 73])
  and debugging (e.g., [43, 49, 54, 71]).

### Improving the Reliability of Deep Learning Software Systems (Doctor Thesis)

- We choose image classification architectures as they are often used as test subjects in recent SE papers that test [86, 146,
  151,182,220,231,232,235,251,258], verify [93,177], and improve [82,119,152,237,249,254]
  DL models and libraries

### Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications (ACM Transactions on Software Engineering and Methodology '23)

- Differential Verification of DNN models: ReluDiff [53] and its following work [54] share certain similar objectives with our approach although it is not a
  testing technique. It leverages the structural and behavioral similarities of the two closely related networks in
  parallel, to verify whether the output diference of the two models are within the speciication. In the evaluation,
  they use the pairs of compressed model and the original model as subjects.
  Our work difers from ReluDiff in two ways. First, **ReluDiff can only be used in forward neural networks
  with relu activation function for both compressed and original models.** This limits its application scenarios.
  Sophisticated DNN models usually contain convolutional layers and recurrent layers. The advantage of DFLARE
  is that it makes no assumption on the model architecture, making it applicable to a wide range of application
  scenarios. Second, **ReluDiff needs to know the architectures and weights of DNN models for veriication, while
  DFLARE works for black-box models.**
